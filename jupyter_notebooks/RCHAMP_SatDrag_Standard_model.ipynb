{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the needed packages and subpackages. \n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jul 14 09:19:54 2022\n",
    "\n",
    "@author: vivianliu\n",
    "adopted for jupyter notebook by Alexa Halford\n",
    "\"\"\"\n",
    "#from dask.distributed import Client\n",
    "\n",
    "#client = Client(n_workers=2, threads_per_worker=2, memory_limit=\"1GB\")\n",
    "#client\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are putting in the information that needs to be hardcoded into \n",
    "# the jupyter notbook runs. \n",
    "\n",
    "#   data: a data frame or string pathname type\n",
    "data = \"../../../Data/combined_data_all_reduced_omni.csv\"\n",
    "\n",
    "#   target_variable: string with target variable name\n",
    "target_variable = '400kmDensity'\n",
    "\n",
    "#Below is our \"Standard model\" for what we want to compare to. \n",
    "#   features (optional): takes a list type with the names of all the \n",
    "#                        variables to include. Default is all\n",
    "Stand_features = [\"DAILY_F10.7_\", \"MagTime\",\"SLat\", \"3-H_KP*10_\"] #\"DipLat\", \n",
    "                  #\"SYM/H_INDEX_nT\"]#, \"1-M_AE_nT\", \"3-H_KP*10_\"]\n",
    "\n",
    "#Models to add? \n",
    "#need to create the slope of Sym-H \n",
    "#GV4_features = [\"DAILY_F10.7_\", \"MagTime\",\"SLat\", \"SYM/H_INDEX_nT\", \"Slope_Sym_H\"]\n",
    "#need to add in variations of sort of latitude, magnetic lat, geographic lat, solar zenith angle, etc\n",
    "\n",
    "#There are some features we want for plotting - like date - \n",
    "#but don't want for creating the model so we identify those here. \n",
    "drop_features = [\"year\", \"hour\", \"minute\", \"second\"] #None\n",
    "\n",
    "\n",
    "#   estimators (optional): integer for number of estimators in \n",
    "#                       random forest. Default is 150\n",
    "estimators = 150\n",
    "\n",
    "\n",
    "#   TrainUpTo (optional): A 4 digit year input that allows users to \n",
    "#                       select training data before inputted year\n",
    "TrainUpTo = 2008 #None\n",
    "\n",
    "\n",
    "#   rdm_state (optional): integer for random state of random \n",
    "#                        forest regression. Defult is 16\n",
    "rdm_state = 16\n",
    "\n",
    "\n",
    "#   test_portion (optional): float between 0 and 1 for \n",
    "#                         proportion of data used for test. \n",
    "#                         Default is 0.25\n",
    "test_portion = 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further preparing the data \n",
    "\n",
    "#Create a dataframe based on data input method\n",
    "#if (type(data) == pd.core.frame.DataFrame):\n",
    "#    merged_df = data;\n",
    "#elif (type(data) == str):\n",
    "#    merged_df = pd.read_csv(data)\n",
    "\n",
    "merged_df = pd.read_csv(data)\n",
    "a = np.array(merged_df.keys())\n",
    "#merged_df = dd.read_csv(data)\n",
    "    \n",
    "#Sort by data for easier reading\n",
    "merged_df = merged_df.sort_values(by = \"Datetime\")\n",
    "merged_df = merged_df.reset_index(drop = True)\n",
    "    \n",
    "#Get rid of any rows outside of expected date range\n",
    "#This needed to be done as there were some bad date and data\n",
    "merged_df = merged_df[~(merged_df[\"Datetime\"] < '2002-05-01')]\n",
    "\n",
    "#Remove datetime column for random forest\n",
    "merged_df = merged_df.drop(\"Datetime\", axis = 1)\n",
    "\n",
    "#Set target and feature variables\n",
    "target = merged_df[target_variable]\n",
    "target = target*(10**12)\n",
    "merged_df = merged_df.drop(target_variable, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are starting to build up the different models by making \n",
    "#sure they have the right inputs/outputs. \n",
    "#Adjust features being used based on user input\n",
    "\n",
    "#First we start with the 'standard' model\n",
    "#Stand_list = list(merged_df.columns)\n",
    "STmodel_features = [\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\"]\n",
    "for element in Stand_features:\n",
    "    STmodel_features.append(element)\n",
    "Stand_list = STmodel_features\n",
    "merged_df = merged_df[Stand_list]\n",
    "a = merged_df.keys()\n",
    "for i in range(len(Stand_list)):\n",
    "    if a[i] in Stand_list:\n",
    "        print(a[i], 'is being kept')\n",
    "    else:\n",
    "        del Stand_list[a[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Set training and testing groups###\n",
    "    \n",
    "#If no testing set specified, create random testing and training groups\n",
    "if (TrainUpTo == None):\n",
    "    train_features, test_features, train_target, test_target = train_test_split(merged_df, target, test_size = test_portion, random_state = rdm_state)\n",
    "else:\n",
    "    #Choose training and testing data\n",
    "    train_features = merged_df[merged_df.year <= TrainUpTo]\n",
    "    size = len(train_features.index)\n",
    "    test_features = merged_df.iloc[size:]\n",
    "    train_target = target.iloc[0:size]\n",
    "    test_target = target.iloc[size:]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new variables to hold current dataframe which has datetime values in case they are dropped\n",
    "#Used for graphing predicted and true values based on date\n",
    "graph_df = merged_df\n",
    "Stand_list_u = Stand_list.copy()\n",
    "test_features_u = test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features that user specifies so that they aren't included in the random forest\n",
    "if (drop_features != None):\n",
    "    train_features = train_features.drop(drop_features, axis = 1)\n",
    "    test_features = test_features.drop(drop_features, axis = 1)   \n",
    "    for element in drop_features:\n",
    "        Stand_list.remove(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and fit the model\n",
    "rf = RandomForestRegressor(n_estimators = estimators, random_state = rdm_state)\n",
    "rf.fit(train_features, train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions and calculate error\n",
    "predictions = rf.predict(test_features)\n",
    "\n",
    "#Print the mean absolute error\n",
    "mean_abs_error = mean_absolute_error(test_target, predictions)\n",
    "print(\"\\nMean Absolute Error: \", mean_abs_error, \" kg/m^3.\")\n",
    "\n",
    "#Print mean absolute percentage error\n",
    "mape = mean_absolute_percentage_error(test_target, predictions)\n",
    "print(\"Mean Absolute Percentage Error: \", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print r-squared score of model\n",
    "score = r2_score(test_target, predictions)\n",
    "#print(\"Score: \", score)\n",
    "\n",
    "#Examine feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(Stand_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "print('For the standard model')\n",
    "print(\"Score: \", score)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "print()\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create arrays for the datetime values for the true data\n",
    "months = graph_df.iloc[:, Stand_list_u.index('month')]\n",
    "days = graph_df.iloc[:, Stand_list_u.index('day')]\n",
    "years = graph_df.iloc[:, Stand_list_u.index('year')]\n",
    "hours = graph_df.iloc[:, Stand_list_u.index('hour')]\n",
    "minutes = graph_df.iloc[:, Stand_list_u.index('minute')]\n",
    "seconds = graph_df.iloc[:, Stand_list_u.index('second')]\n",
    "    \n",
    "#Convert datetime arrays to datetime type\n",
    "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) + \" \" + str(int(hour)) + \":\" + str(int(minute)) + \":\" + str(int(second)) for year, month, day, hour, minute, second in zip(years, months, days, hours, minutes, seconds)]\n",
    "dates = [dt.datetime.strptime(date, '%Y-%m-%d %H:%M:%S') for date in dates]\n",
    "    \n",
    "#Create dataframe using datetime and target data\n",
    "true_data = pd.DataFrame(data = {'date': dates, 'actual': target})\n",
    "true_data = true_data.sort_values(by = \"date\")\n",
    "true_data[\"actual\"] = true_data[\"actual\"] / (10**12)\n",
    "    \n",
    "#Create arrays for the datetime values for the predicted data\n",
    "months = test_features_u.iloc[:, Stand_list_u.index('month')]\n",
    "days = test_features_u.iloc[:, Stand_list_u.index('day')]\n",
    "years = test_features_u.iloc[:, Stand_list_u.index('year')]\n",
    "hours = test_features_u.iloc[:, Stand_list_u.index('hour')]\n",
    "minutes = test_features_u.iloc[:, Stand_list_u.index('minute')]\n",
    "seconds = test_features_u.iloc[:, Stand_list_u.index('second')]\n",
    "    \n",
    "#Convert datetime arrays to datetime type\n",
    "test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) + \" \" + str(int(hour)) + \":\" + str(int(minute)) + \":\" + str(int(second)) for year, month, day, hour, minute, second in zip(years, months, days, hours, minutes, seconds)]\n",
    "test_dates = [dt.datetime.strptime(date, '%Y-%m-%d %H:%M:%S') for date in test_dates]\n",
    "    \n",
    "#Make a new dataframe with prediction data\n",
    "prediction_data = pd.DataFrame(data = {\"dates\": test_dates, \"predictions\": predictions})\n",
    "prediction_data = prediction_data.sort_values(by = \"dates\")\n",
    "prediction_data[\"predictions\"] = prediction_data[\"predictions\"] / (10**12)\n",
    "    \n",
    "#Plot the true values in blue\n",
    "plt.plot(true_data[\"date\"], true_data[\"actual\"], \"b-\", label = \"actual\")\n",
    "#Plot predicted values in magenta\n",
    "plt.plot(prediction_data[\"dates\"], prediction_data[\"predictions\"], \"mo\", label = \"predicted\", markersize = 3)\n",
    "    \n",
    "    \n",
    "#Label Plot\n",
    "plt.xticks(rotation = 60)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"400 km Density\")\n",
    "plt.title(\"Actual and Predicted Values of\\nRandom Forest for 400km Density\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
